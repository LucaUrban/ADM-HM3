{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk; from nltk.corpus import stopwords; from nltk.stem import PorterStemmer\n",
    "import heapq\n",
    "\n",
    "#==========================================================================================================================\n",
    "# Part related to the crawling of wikipedia\n",
    "#==========================================================================================================================\n",
    "\n",
    "# doing the request to the web page with the films\n",
    "request = requests.get(\"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html\")\n",
    "soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "# extract all the links that are into the page and insert all the links into a list\n",
    "film_list = soup.select(\"a\")\n",
    "for i in range(len(film_list)):\n",
    "    film_list[i] = film_list[i].get(\"href\")\n",
    "\n",
    "# for each link that is into the list created previously extract the HTML code related to that link and insert it into a\n",
    "# file created at the moment into the position specified by the url_file variable\n",
    "for i in range(len(film_list)):\n",
    "    url_file = \"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles3\\\\article_\" + str(i+20001) + \".html\"\n",
    "    file = open(url_file, \"w\", encoding=\"utf-8\")\n",
    "    file.write(str(requests.get(film_list[i]).text))\n",
    "    file.close()\n",
    "\n",
    "#==========================================================================================================================\n",
    "# That part extract all needed informations from the crawled pages\n",
    "#==========================================================================================================================\n",
    "\n",
    "# extracting data from crawled HTML pages and transform it into a film.tsv file with the extracted informations as specified\n",
    "# into the requests\n",
    "for i in range(len(film_list)):\n",
    "    # inizializing variables\n",
    "    flag = 0; \n",
    "    director = \"NA\"; producer = \"NA\"; writer = \"NA\"; starring = \"NA\"; music = \"NA\"; release_date = \"NA\"; runtime = \"NA\"; \n",
    "    country = \"NA\"; language = \"NA\"; budget = \"NA\"\n",
    "    \n",
    "    # creating url, open file and parsing it\n",
    "    url = \"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\article_\" + str(i+1) + \".html\"\n",
    "    fileIn = open(url, \"r\", encoding=\"utf-8\")\n",
    "    fileOut = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\film\" + str(i+1) + \".tsv\", \"a\", encoding=\"utf-8\")\n",
    "    film = BeautifulSoup(fileIn.read(), \"html.parser\")\n",
    "    \n",
    "    # extract title of the page and the film\n",
    "    list_h1 = film.select(\"h1\")\n",
    "    for h1 in list_h1:\n",
    "        if h1.get(\"id\") == \"firstHeading\":\n",
    "            fTitle = h1.text\n",
    "            \n",
    "    # this part trys to extract the intro and plot of the film, if they aren't into the page they are set to NA\n",
    "    try:\n",
    "        intro = film.select(\"p\")[0].text\n",
    "        plot = film.select(\"p\")[1].text\n",
    "    except:\n",
    "        intro = \"NA\"; plot = \"NA\"\n",
    "    finally:\n",
    "    \n",
    "        # search and then extract the required values from the infobox table related to the film\n",
    "        fTables = film.select(\"table\")\n",
    "        for table in fTables:\n",
    "            if table.get(\"class\") != None:\n",
    "                if \"infobox\" in table.get(\"class\"):\n",
    "                    rows = table.select(\"tr\")\n",
    "                    for row in rows:\n",
    "                        for el in row.select(\"th\"):\n",
    "                            if \"class\" in el.attrs.keys():\n",
    "                                if \"summary\" in el.attrs[\"class\"]:\n",
    "                                    flag = 1\n",
    "                                    summary = el.text\n",
    "                                else:\n",
    "                                    summary = \"NA\"\n",
    "                            if el.text == \"Directed by\":\n",
    "                                director = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Produced by\":\n",
    "                                producer = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Written by\":\n",
    "                                writer = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Starring\":\n",
    "                                starring = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Music by\":\n",
    "                                music = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Release date\":\n",
    "                                release_date = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Running time\":\n",
    "                                runtime = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Country\":\n",
    "                                country = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Language\":\n",
    "                                language = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Budget\":\n",
    "                                budget = row.select(\"td\")[pos].text\n",
    "                            pos += 1\n",
    "                        pos = 0\n",
    "\n",
    "        # write into the result file all the important data extracted according to the format of a tsv file\n",
    "        if flag == 0: summary = \"NA\"\n",
    "        fileOut.write(fTitle + \" \\t \" + intro + \" \\t \" + plot + \" \\t \" + summary + \" \\t \" + director + \" \\t \" + producer + \" \\t \" + \n",
    "                      writer + \" \\t \" + starring + \" \\t \" + music + \" \\t \" + release_date + \" \\t \" + runtime + \" \\t \" + country +\n",
    "                      \" \\t \" + language + \" \\t \" + budget)\n",
    "    \n",
    "    # close the files that aren't used\n",
    "    fileIn.close(); fileOut.close()\n",
    "\n",
    "#==========================================================================================================================\n",
    "# This park makes the indexing of the extracted informations \n",
    "#==========================================================================================================================\n",
    "\n",
    "# declarating global variables\n",
    "rev_index_App = {}; rev_index = {}\n",
    "commonWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# does a first reverse indexing, at the end the result is represented by a dictionary with words as a key and the value\n",
    "# associated is a list of integers where each integer represent the number of the page where the word compares. Ex:\n",
    "# {\"word_1\": [1, 2, 4, 5], \"word_2\": [2, 5, 6] ...}\n",
    "# so the word_a is into the first, second, fourth and fifth pages, the word_2 is into second, fifth and sixth page and so on\n",
    "for i in range(1, 10000):\n",
    "    # opening files and variables\n",
    "    fileIn = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\film\" + str(i) + \".tsv\", encoding=\"utf-8\")\n",
    "    intro_plot = []\n",
    "    fileText = fileIn.read()\n",
    "    \n",
    "    # extract the summary, intro and the plot of each film article\n",
    "    for j in range(3):\n",
    "        intro_plot.append(fileText[:fileText.find(\"\\t\")].strip())\n",
    "        fileText = fileText[fileText.find(\"\\t\") + 2:]\n",
    "\n",
    "    # first removes the summary because it isn't required for the indexing of the words\n",
    "    # then does a parsing of the string represented by the el variable transforming it into a list of words then\n",
    "    # controls if the word is \"legit\" and insert it into a list that will be used after to construct a first result\n",
    "    # dictionary as exposed into the comment that begins on the row 10\n",
    "    intro_plot.pop(0)\n",
    "    for el in intro_plot:\n",
    "        listaParole = nltk.word_tokenize(el); listaParoleAcc = []\n",
    "        for j in range(len(listaParole)):\n",
    "            if listaParole[j].isalpha() == True and listaParole[j].lower() not in commonWords:\n",
    "                listaParoleAcc.append(listaParole[j].lower())\n",
    "        for word in listaParoleAcc:\n",
    "            if word not in rev_index_App.keys():\n",
    "                rev_index_App[word] = [i]\n",
    "            else:\n",
    "                if i not in rev_index_App[word]:\n",
    "                    rev_index_App[word].append(i)\n",
    "    fileIn.close()\n",
    "\n",
    "# this part creates a matrix with 2 columns and 10000 rows, each row contains the id of the word and the word associated\n",
    "# Ex: [[1, word_1], [2, word_2], [3, word_3] ...]\n",
    "dict_id = []\n",
    "for i in range(len(rev_index_App.keys())):\n",
    "    dict_id.append([i+1, list(rev_index_App.keys())[i]])\n",
    "\n",
    "# this part creates a new dictionary where the keys aren't the words but their index\n",
    "# Ex: {1: [1, 2, 3, 4], 2: [2, 4, 6, 7] ...}\n",
    "value = list(rev_index_App.values())\n",
    "for i in range(len(rev_index_App.keys())):\n",
    "    rev_index[i+1] = value[i]\n",
    "\n",
    "# writes into two different files the matrix with the \"conversion table\" from an id to his word and the reverse index\n",
    "# dictionary\n",
    "fileIds = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\vocabulary.txt\", \"w\", encoding=\"utf-8\")\n",
    "fileIds.write(str(dict_id))\n",
    "fileIds.close()\n",
    "fileDict = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\reverseIndex.txt\", \"w\", encoding=\"utf-8\")\n",
    "fileDict.write(str(rev_index))\n",
    "fileDict.close()\n",
    "\n",
    "#==========================================================================================================================\n",
    "# This part makes a film research using weighted scores \n",
    "#==========================================================================================================================\n",
    "\n",
    "# this function controls if at least one word of the query matches with the words contained into an element of the film, if \n",
    "# this happen returnd a positive score, otherwise it returns 0\n",
    "def assPunt(lisParAcc, lisParQuery, i):\n",
    "    for q in lisParQuery:\n",
    "        if q.lower() in lisParAcc:\n",
    "            if 1 <= i <= 3:\n",
    "                return 19\n",
    "            else:\n",
    "                return 4\n",
    "    return 0\n",
    "\n",
    "# creation of global variables\n",
    "firstRes = {}; res = []; commonWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# handles the query and makes the reseach with the score\n",
    "query = input(\"Inserisci le parole chiave della tua ricerca: \")\n",
    "query = query.split(\" \")\n",
    "for i in range(10000):\n",
    "    fileIn = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles1\\\\film\" + str(i+1) + \".tsv\", encoding=\"utf-8\")\n",
    "    info = []\n",
    "    fileText = fileIn.read()\n",
    "    tot = 0\n",
    "\n",
    "    while fileText.find(\"\\t\") != -1:\n",
    "        info.append(fileText[:fileText.find(\"\\t\")].strip())\n",
    "        fileText = fileText[fileText.find(\"\\t\") + 2:]\n",
    "    info.append(fileText.strip())\n",
    "\n",
    "    # gives a score to the document and creates a dictionary with the number of the documents with highter score\n",
    "    for el in info:\n",
    "        cont = 1\n",
    "        listaParole = nltk.word_tokenize(el); listaParoleAcc = []\n",
    "        for word in listaParole:\n",
    "            word = word.lower()\n",
    "            if word.isalpha() == True and word not in commonWords and PorterStemmer().stem(word) not in listaParoleAcc:\n",
    "                listaParoleAcc.append(PorterStemmer().stem(word))\n",
    "        tot += assPunt(listaParoleAcc, query, cont)\n",
    "        cont += 1\n",
    "\n",
    "    if len(firstRes) < 10:\n",
    "        firstRes[i+1] = round(tot/101, 4)\n",
    "    else:\n",
    "        if round(tot/101, 4) > min(firstRes.values()):\n",
    "            for key in firstRes.keys():\n",
    "                if firstRes[key] == min(firstRes.values()):\n",
    "                    remKey = key\n",
    "            firstRes.pop(remKey)\n",
    "            firstRes[i+1] = round(tot/101, 4)\n",
    "\n",
    "# creates the heap tree and extract the values from the tree (in practical it makes an heapSort)\n",
    "heap = list(firstRes.values())\n",
    "heapq.heapify(heap)\n",
    "for i in range(len(firstRes)):\n",
    "    res.insert(0, heapq.heappop(heap))\n",
    "\n",
    "# search the number of the document with a certain scores and then print the required elements(title, intro, link, and score)\n",
    "for el in res:\n",
    "    for key, value in firstRes.items():\n",
    "        if value == el:\n",
    "            fileIn = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles1\\\\film\" + str(key) + \".tsv\", encoding=\"utf-8\")\n",
    "            fileText = fileIn.read()\n",
    "            s = \"\"\n",
    "            for j in range(2):\n",
    "                s += fileText[:fileText.find(\"\\t\")].strip() + \"\\t\"\n",
    "                fileText = fileText[fileText.find(\"\\t\") + 2:]\n",
    "            s += film_list[key-1] + \"\\t\" + str(value)\n",
    "            print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
