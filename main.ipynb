{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import nltk; from nltk.corpus import stopwords; from nltk.stem import PorterStemmer\n",
    "import heapq\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#==========================================================================================================================\n",
    "# Part related to the crawling of wikipedia\n",
    "#==========================================================================================================================\n",
    "\n",
    "# doing the request to the web page with the films\n",
    "film_list = []; app = []\n",
    "for i in range(3):\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies\" + str(i+1) \n",
    "                           + \".html\")\n",
    "    soup = BeautifulSoup(request.text, 'html.parser')\n",
    "\n",
    "    # extract all the links that are into the page and insert all the links into a list\n",
    "    app = soup.select(\"a\")\n",
    "    for i in range(len(app)):\n",
    "        film_list.append(app[i].get(\"href\"))\n",
    "    \n",
    "\n",
    "# for each link that is into the list created previously extract the HTML code related to that link and insert it into a\n",
    "# file created at the moment into the position specified by the url_file variable\n",
    "for i in range(len(film_list)):\n",
    "    url_file = \"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles3\\\\article_\" +str(i+1)+ \".html\"\n",
    "    file = open(url_file, \"w\", encoding=\"utf-8\")\n",
    "    file.write(str(requests.get(film_list[i]).text))\n",
    "    file.close()\n",
    "\n",
    "#==========================================================================================================================\n",
    "# That part extract all needed informations  from the crawled pages\n",
    "#==========================================================================================================================\n",
    "\n",
    "# extracting data from crawled HTML pages and transform it into a film.tsv file with the extracted informations as specified\n",
    "# into the requests\n",
    "for i in range(len(film_list)):\n",
    "    # inizializing variables\n",
    "    flag = 0; \n",
    "    director = \"NA\"; producer = \"NA\"; writer = \"NA\"; starring = \"NA\"; music = \"NA\"; release_date = \"NA\"; runtime = \"NA\"; \n",
    "    country = \"NA\"; language = \"NA\"; budget = \"NA\"\n",
    "    \n",
    "    # creating url, open file and parsing it\n",
    "    url = \"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\article_\" + str(i+1) + \".html\"\n",
    "    fileIn = open(url, \"r\", encoding=\"utf-8\")\n",
    "    fileOut = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\film\" + str(i+1) + \".tsv\", \"a\", encoding=\"utf-8\")\n",
    "    film = BeautifulSoup(fileIn.read(), \"html.parser\")\n",
    "    \n",
    "    # extract title of the page and the film\n",
    "    list_h1 = film.select(\"h1\")\n",
    "    for h1 in list_h1:\n",
    "        if h1.get(\"id\") == \"firstHeading\":\n",
    "            fTitle = h1.text\n",
    "            \n",
    "    # this part trys to extract the intro and plot of the film, if they aren't into the page they are set to NA\n",
    "    try:\n",
    "        intro = film.select(\"p\")[0].text\n",
    "        plot = film.select(\"p\")[1].text\n",
    "    except:\n",
    "        intro = \"NA\"; plot = \"NA\"\n",
    "    finally:\n",
    "    \n",
    "        # search and then extract the required values from the infobox table related to the film\n",
    "        fTables = film.select(\"table\")\n",
    "        for table in fTables:\n",
    "            if table.get(\"class\") != None:\n",
    "                if \"infobox\" in table.get(\"class\"):\n",
    "                    rows = table.select(\"tr\")\n",
    "                    for row in rows:\n",
    "                        for el in row.select(\"th\"):\n",
    "                            if \"class\" in el.attrs.keys():\n",
    "                                if \"summary\" in el.attrs[\"class\"]:\n",
    "                                    flag = 1\n",
    "                                    summary = el.text\n",
    "                                else:\n",
    "                                    summary = \"NA\"\n",
    "                            if el.text == \"Directed by\":\n",
    "                                director = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Produced by\":\n",
    "                                producer = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Written by\":\n",
    "                                writer = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Starring\":\n",
    "                                starring = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Music by\":\n",
    "                                music = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Release date\":\n",
    "                                release_date = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Running time\":\n",
    "                                runtime = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Country\":\n",
    "                                country = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Language\":\n",
    "                                language = row.select(\"td\")[pos].text\n",
    "                            if el.text == \"Budget\":\n",
    "                                budget = row.select(\"td\")[pos].text\n",
    "                            pos += 1\n",
    "                        pos = 0\n",
    "\n",
    "        # write into the result file all the important data extracted according to the format of a tsv file\n",
    "        if flag == 0: summary = \"NA\"\n",
    "        fileOut.write(fTitle + \" \\t \" + intro + \" \\t \" + plot + \" \\t \" + summary + \" \\t \" + director + \" \\t \" + producer + \" \\t \" + \n",
    "                      writer + \" \\t \" + starring + \" \\t \" + music + \" \\t \" + release_date + \" \\t \" + runtime + \" \\t \" + country +\n",
    "                      \" \\t \" + language + \" \\t \" + budget)\n",
    "    \n",
    "    # close the files that aren't used\n",
    "    fileIn.close(); fileOut.close()\n",
    "\n",
    "#==========================================================================================================================\n",
    "# This park makes the indexing of the extracted informations \n",
    "#==========================================================================================================================\n",
    "\n",
    "# declarating global variables\n",
    "rev_index_App = {}; rev_index = {}\n",
    "commonWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# does a first reverse indexing, at the end the result is represented by a dictionary with words as a key and the value\n",
    "# associated is a list of integers where each integer represent the number of the page where the word compares. Ex:\n",
    "# {\"word_1\": [1, 2, 4, 5], \"word_2\": [2, 5, 6] ...}\n",
    "# so the word_a is into the first, second, fourth and fifth pages, the word_2 is into second, fifth and sixth page and so on\n",
    "for i in range(1, 10000):\n",
    "    # opening files and variables\n",
    "    fileIn = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\film\" + str(i) + \".tsv\", encoding=\"utf-8\")\n",
    "    intro_plot = []\n",
    "    fileText = fileIn.read()\n",
    "    \n",
    "    # extract the summary, intro and the plot of each film article\n",
    "    for j in range(3):\n",
    "        intro_plot.append(fileText[:fileText.find(\"\\t\")].strip())\n",
    "        fileText = fileText[fileText.find(\"\\t\") + 2:]\n",
    "\n",
    "    # first removes the summary because it isn't required for the indexing of the words\n",
    "    # then does a parsing of the string represented by the el variable transforming it into a list of words then\n",
    "    # controls if the word is \"legit\" and insert it into a list that will be used after to construct a first result\n",
    "    # dictionary as exposed into the comment that begins on the row 10\n",
    "    intro_plot.pop(0)\n",
    "    for el in intro_plot:\n",
    "        listaParole = nltk.word_tokenize(el); listaParoleAcc = []\n",
    "        for j in range(len(listaParole)):\n",
    "            if listaParole[j].isalpha() == True and listaParole[j].lower() not in commonWords:\n",
    "                listaParoleAcc.append(listaParole[j].lower())\n",
    "        for word in listaParoleAcc:\n",
    "            if word not in rev_index_App.keys():\n",
    "                rev_index_App[word] = [i]\n",
    "            else:\n",
    "                if i not in rev_index_App[word]:\n",
    "                    rev_index_App[word].append(i)\n",
    "    fileIn.close()\n",
    "\n",
    "# this part creates a matrix with 2 columns and 10000 rows, each row contains the id of the word and the word associated\n",
    "# Ex: [[1, word_1], [2, word_2], [3, word_3] ...]\n",
    "dict_id = []\n",
    "for i in range(len(rev_index_App.keys())):\n",
    "    dict_id.append([i+1, list(rev_index_App.keys())[i]])\n",
    "\n",
    "# this part creates a new dictionary where the keys aren't the words but their index\n",
    "# Ex: {1: [1, 2, 3, 4], 2: [2, 4, 6, 7] ...}\n",
    "value = list(rev_index_App.values())\n",
    "for i in range(len(rev_index_App.keys())):\n",
    "    rev_index[i+1] = value[i]\n",
    "\n",
    "# writes into two different files the matrix with the \"conversion table\" from an id to his word and the reverse index\n",
    "# dictionary\n",
    "fileIds = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\vocabulary.txt\", \"w\", encoding=\"utf-8\")\n",
    "fileIds.write(str(dict_id))\n",
    "fileIds.close()\n",
    "fileDict = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\reverseIndex.txt\", \"w\", encoding=\"utf-8\")\n",
    "fileDict.write(str(rev_index))\n",
    "fileDict.close()\n",
    "\n",
    "#==========================================================================================================================\n",
    "# This part makes a film research using cosine similarity from each document \n",
    "#==========================================================================================================================\n",
    "\n",
    "#to execute the query in an easiest way\n",
    "dict_id_1 = {}\n",
    "for i in range(len(dict_id)):\n",
    "    dict_id_1[dict_id[i][1]] = i+1 #form nested list to dict\n",
    "    \n",
    "num_query = []\n",
    "\n",
    "query = input('Insert your search: ').lower()  # so we work with every word in the same format\n",
    "query = nltk.word_tokenize(query)              # list of tokenized words of the query \n",
    "\n",
    "for el in query:\n",
    "    if (PorterStemmer().stem(el)) in dict_id_1:               # stemmed query\n",
    "        num_query.append(dict_id_1[PorterStemmer().stem(el)]) # list with query entry converted in the same number given to \n",
    "                                                              # the words in my database\n",
    "\n",
    "query_dictionary = {}   # use this dict to select only words and related docs that i want to check with te query\n",
    "nested_list = []\n",
    "\n",
    "for el in num_query:\n",
    "    if el in rev_index.keys():\n",
    "        query_dictionary[el] = rev_index[el] # a new dict with only the key value I need\n",
    "        nested_list.append(rev_index[el])\n",
    "\n",
    "# Now we procced doing the set of sublist that have the documents in wich is contained each of the query's word.\n",
    "# So the intersection between them, is a list in wich there are only the documents that have all the words requested!\n",
    "\n",
    "s = set(i for sub in nested_list for i in sub) # unique list of all documents that have all query\n",
    "query_result = [i for i in s if all(i in sub for sub in nested_list)] # find all docs that have al words of query doing the\n",
    "                                                                      # intersection of sets\n",
    "# Now we have to create a unique list of film links, because before we worked in parallel\n",
    "\n",
    "query_df = pd.DataFrame(); l = []\n",
    "\n",
    "for i in sorted(query_result): # for film in series, respect chronological order\n",
    "    url = \"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles\\\\film\" + str(i) + \".tsv\"\n",
    "    fileIn = open(url,'r',encoding='utf-8')\n",
    "    s = fileIn.read()\n",
    "    s1 = s.replace('\\t \\n \\t','\\t') #'\\n' gives a problem about splitting the text \n",
    "    s_splitted = s1.split('\\t')\n",
    "\n",
    "    l.append([s_splitted[0], s_splitted[1], film_list[i-1]])\n",
    "    \n",
    "query_df = pd.DataFrame(l)\n",
    "query_df.columns=['Title','Intro','Wikipedia link']\n",
    "\n",
    "# Now for this query we need to do some changes on the previous query code\n",
    "\n",
    "dict_id = []\n",
    "dictionary_id = {} \n",
    "for i in range(len(rev_index_App.keys())):\n",
    "    a = list(rev_index_App.keys())[i]\n",
    "    dictionary_id[a]= i+1\n",
    "    \n",
    "word_id_list = [] # each sub has the words of a document, stored by their Id\n",
    "\n",
    "for sub in range(len(words_doc_i)): # each sublist\n",
    "    l_temp=[]\n",
    "    for word in range(len(words_doc_i[sub])): # each word\n",
    "        if words_doc_i[sub][word] in dictionary_id.keys():\n",
    "            l_temp.append(dictionary_id[words_doc_i[sub][word]])\n",
    "    word_id_list.append(l_temp)\n",
    "    \n",
    "tfDict_final = {}\n",
    "for i in range(docs):\n",
    "    tfDict = {} #need one per document\n",
    "    words =  word_id_list[i]\n",
    "    words_number = len(word_id_list[i])    #counting all the words inisde the doc\n",
    "    \n",
    "    numOfWords_i = dict.fromkeys(set(words), 0) #every word in the document with start occurency 0\n",
    "    for word in words:\n",
    "        numOfWords_i[word] += 1 #dict with words of document(i) and their occurences\n",
    "    \n",
    "    for word in numOfWords_i:\n",
    "        try:\n",
    "            tfDict[word] = round(numOfWords_i[word] / float(words_number), 4) #appplication of the rule to calculate 'tf'\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    tfDict_final[i]=tfDict\n",
    "\n",
    "value = list(rev_index_App.values())\n",
    "for i in range(len(rev_index_App.keys())):\n",
    "    rev_index[i+1] = value[i]               # dict with each word and in which docs we can find it \n",
    "    \n",
    "all_set_occ = {} # the frequency of each word over all the doc set\n",
    "\n",
    "for key in rev_index:\n",
    "    summary = len(rev_index[key]) # rev_index[key] is a list of all docs where is a certain word, so we need the lenght of this \n",
    "                                  # list\n",
    "    newkey = int(key)\n",
    "    all_set_occ[newkey] = round(math.log(docs/summary), 4) \n",
    "\n",
    "# Now we'll build a new dictionary, in wich for each key (word) we can see in wich docs is contained and also for each one \n",
    "# of the the tf-idf score.\n",
    "\n",
    "word_list = list(dictionary_id.values())\n",
    " \n",
    "Rev_Index_1 = {}\n",
    "    \n",
    "for word in word_list:\n",
    "    l_doc = rev_index[word]   #list of all documents with that word\n",
    "    for doc in l_doc:\n",
    "        words_tf = tfDict_final[doc]         #all the words in Doc(i) with their tf\n",
    "        for doc_word in words_tf.keys():     #for each word of each doc\n",
    "            Idf = all_set_occ[doc_word]           #find the idf of that word\n",
    "            TfIdf = round((words_tf[doc_word] * Idf),3) #calculate the tf-idf score\n",
    "            \n",
    "            if doc_word not in Rev_Index_1.keys(): #if the word is not in the dict add it and add the first doc with and the tfidf\n",
    "                Rev_Index_1[doc_word] = [(doc,TfIdf)]\n",
    "            else:\n",
    "                if (doc,TfIdf) not in Rev_Index_1[doc_word]: #if the word is already in the dict we update the values\n",
    "                    Rev_Index_1[doc_word].append((doc,TfIdf))\n",
    "\n",
    "queryIn = input('Insert your search :').lower()\n",
    "elems_query = nltk.word_tokenize(queryIn)  #list of words of the query\n",
    "numerical_query = []\n",
    "\n",
    "\n",
    "for el in elems_query:\n",
    "    if (PorterStemmer().stem(el)) in dict_id_1:               #stemmed query\n",
    "        numerical_query.append(dict_id_1[PorterStemmer().stem(el)]) #list with query entry converted in the same number given to the words in my database\n",
    "\n",
    "query_dict = {}   #use this dict to select only rows and columns I want to checj\n",
    "list_nested = []\n",
    "l_tfidf = []\n",
    "dict_word_l_tfidf = {}\n",
    "\n",
    "for el in numerical_query:\n",
    "    if el in Rev_Index_1.keys():\n",
    "        l_tfidf_temp = []\n",
    "        query_dict[el] = Rev_Index_1[el]   # a dict with only the values we need to execute the query\n",
    "        list_nested.append(Rev_Index_1[el]) # a list with only the values we need to execute the query\n",
    "        \n",
    "        docs_list = query_dict[el]\n",
    "        \n",
    "        for value in docs_list:\n",
    "            l_tfidf_temp.append(value[1]) \n",
    "    l_tfidf.append(l_tfidf_temp)\n",
    "    \n",
    "    dict_word_l_tfidf[el] = l_tfidf_temp  #new dict, keys are words and values the list of their tfidf\n",
    "    \n",
    "set_list = []\n",
    "docs_nested_list = []\n",
    "\n",
    "for sub in list_nested: #sub is formed by a list of documents\n",
    "    l = []\n",
    "    for elem in sub:\n",
    "        set_list.append(elem[0])\n",
    "        l.append(elem[0])\n",
    "    \n",
    "    docs_nested_list.append(l)\n",
    "\n",
    "set_list = set(set_list)\n",
    "\n",
    "query_results = [i for i in set_list if all(i in sub for sub in docs_nested_list)] # find all docs that have al words of \n",
    "                                                                                   # query doing the intersection of sets\n",
    "    \n",
    "query_dict_tfidf = {}\n",
    "for key in query_dict:\n",
    "    num_of_docs = dict.fromkeys(total_docs, 0)\n",
    "    for doc_tfidf in query_dict[key]:\n",
    "        num_of_docs[doc_tfidf[0]] += doc_tfidf[1]\n",
    "    \n",
    "    doc_list = list(num_of_docs.values())\n",
    "    query_dict_tfidf[key] = doc_list    \n",
    "\n",
    "# this dic has every word of the query as a key, and as a values a list of tfidf for each document, from the first to the last\n",
    "# of the dataset, so we have a value if the word is inside a doc or 0 if there isn't. This format is the one we need to compute\n",
    "# cos similarity between query results\n",
    "\n",
    "query_df = pd.DataFrame.from_dict(query_dict_tfidf ) #by index are not ordered \n",
    "\n",
    "query_results_sort = sorted(query_results)\n",
    "query_norm = []\n",
    "for i in query_results_sort:\n",
    "    new = i-1\n",
    "    query_norm.append(new)\n",
    "    \n",
    "newdf_query = query_df.loc[query_norm]\n",
    "\n",
    "query_for_cosine = []\n",
    "for i in numerical_query:\n",
    "    query_for_cosine.append(1) #format needed to perform the cos sim between the query and the dataframe's rows \n",
    "\n",
    "\n",
    "cosine_results = []\n",
    "cosine_results = cosine_similarity([query_for_cosine], newdf_query)\n",
    "cosine_results = cosine_results.tolist()\n",
    "\n",
    "for elem in cosine_results:\n",
    "    el = elem\n",
    "cosine_results_1 = el\n",
    "\n",
    "l_title = []; l_intro = []; l_link = []\n",
    "for i in (query_results_sort): #for film in series, respect chronological order\n",
    "    url = 'C:\\\\Users\\\\Emanuele Fratocchi\\\\Desktop\\\\Data Science\\\\Articles\\\\tsv_HW3\\\\film' + str(i) +'.tsv'\n",
    "    fileIn = open(url,'r',encoding='utf-8')\n",
    "    s = fileIn.read()\n",
    "    s1 = s.replace('\\t \\n \\t','\\t') #'\\n' gives a problem about splitting the text \n",
    "    s_splitted = s1.split('\\t')\n",
    "\n",
    "    l_title.append(s_splitted[0]) #building the columns of the dataframe\n",
    "    l_intro.append(s_splitted[1])\n",
    "    l_link.append(film_list_final[i-1])\n",
    "\n",
    "query_resDic = {}\n",
    "for key_index in range(len(query_results_sort)):\n",
    "    query_resDic[query_results_sort[key_index]] = [l_title[key_index], l_intro[key_index], l_link[key_index],cosine_results_1[key_index]]\n",
    "\n",
    "Res_df = pd.DataFrame(query_resDic)\n",
    "Res_df = Res_df.transpose()    \n",
    "Res_df.columns = ['Title', 'Intro','Wikipedia Link','Cos Similarity']\n",
    "Res_df = Res_df.sort_values('Cos Similarity',ascending = False)\n",
    "Res_df.reset_index(inplace=True)\n",
    "Res_df = Res_df[['Title', 'Intro','Wikipedia Link','Cos Similarity']]\n",
    "\n",
    "if len(query_results_sort) <= 10:\n",
    "    Res_df = Res_df.head()\n",
    "elif len(query_results_sort) > 10:\n",
    "    Res_df = Res_df.head(10)\n",
    "\n",
    "print(Res_df)\n",
    "\n",
    "#==========================================================================================================================\n",
    "# This part makes a film research using weighted scores \n",
    "#==========================================================================================================================\n",
    "\n",
    "# this function controls if at least one word of the query matches with the words contained into an element of the film, if \n",
    "# this happen returnd a positive score, otherwise it returns 0\n",
    "def assPunt(lisParAcc, lisParQuery, i):\n",
    "    for q in lisParQuery:\n",
    "        if q.lower() in lisParAcc:\n",
    "            if 1 <= i <= 3:\n",
    "                return 19\n",
    "            else:\n",
    "                return 4\n",
    "    return 0\n",
    "\n",
    "# creation of global variables\n",
    "firstRes = {}; res = []; commonWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "# handles the query and makes the reseach with the score\n",
    "query = input(\"Inserisci le parole chiave della tua ricerca: \")\n",
    "query = query.split(\" \")\n",
    "for i in range(10000):\n",
    "    fileIn = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles1\\\\film\" + str(i+1) + \".tsv\", encoding=\"utf-8\")\n",
    "    info = []\n",
    "    fileText = fileIn.read()\n",
    "    tot = 0\n",
    "\n",
    "    while fileText.find(\"\\t\") != -1:\n",
    "        info.append(fileText[:fileText.find(\"\\t\")].strip())\n",
    "        fileText = fileText[fileText.find(\"\\t\") + 2:]\n",
    "    info.append(fileText.strip())\n",
    "\n",
    "    # gives a score to the document and creates a dictionary with the number of the documents with highter score\n",
    "    for el in info:\n",
    "        cont = 1\n",
    "        listaParole = nltk.word_tokenize(el); listaParoleAcc = []\n",
    "        for word in listaParole:\n",
    "            word = word.lower()\n",
    "            if word.isalpha() == True and word not in commonWords and PorterStemmer().stem(word) not in listaParoleAcc:\n",
    "                listaParoleAcc.append(PorterStemmer().stem(word))\n",
    "        tot += assPunt(listaParoleAcc, query, cont)\n",
    "        cont += 1\n",
    "\n",
    "    if len(firstRes) < 10:\n",
    "        firstRes[i+1] = round(tot/101, 4)\n",
    "    else:\n",
    "        if round(tot/101, 4) > min(firstRes.values()):\n",
    "            for key in firstRes.keys():\n",
    "                if firstRes[key] == min(firstRes.values()):\n",
    "                    remKey = key\n",
    "            firstRes.pop(remKey)\n",
    "            firstRes[i+1] = round(tot/101, 4)\n",
    "\n",
    "# creates the heap tree and extract the values from the tree (in practical it makes an heapSort)\n",
    "heap = list(firstRes.values())\n",
    "heapq.heapify(heap)\n",
    "for i in range(len(firstRes)):\n",
    "    res.insert(0, heapq.heappop(heap))\n",
    "\n",
    "# search the number of the document with a certain scores and then print the required elements(title, intro, link, and score)\n",
    "for el in res:\n",
    "    for key, value in firstRes.items():\n",
    "        if value == el:\n",
    "            fileIn = open(\"C:\\\\Users\\\\asus\\\\Desktop\\\\Algoritmic methods for data science\\\\ADM-HM3\\\\articles1\\\\film\" + str(key) + \".tsv\", encoding=\"utf-8\")\n",
    "            fileText = fileIn.read()\n",
    "            s = \"\"\n",
    "            for j in range(2):\n",
    "                s += fileText[:fileText.find(\"\\t\")].strip() + \"\\t\"\n",
    "                fileText = fileText[fileText.find(\"\\t\") + 2:]\n",
    "            s += film_list[key-1] + \"\\t\" + str(value)\n",
    "            print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
